{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "383740f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bcp6w\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.6.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.7.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import TFAutoModel, AutoTokenizer, PreTrainedTokenizerFast, TFAutoModelForMaskedLM, TFAutoModelForTokenClassification, TFMT5ForConditionalGeneration\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as kl\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import sentencepiece\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "import tarfile\n",
    "import glob\n",
    "import random\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcc52c0",
   "metadata": {},
   "source": [
    "Tasks:\n",
    "1. import parallel corpus file(s)\n",
    "2. combine sequences with <\\s> in between \n",
    "3. tokenize and pad sequences\n",
    "4. get list of chinese tokens (use .map(regex) to get list; then tokenize it)\n",
    "5. make random token replacements for chinese half (ignoring pad, English, start and end tokens)\n",
    "6. create labels for chinese mistranslation\n",
    "7. write tokenized sequence, labels to csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ce6fce",
   "metadata": {},
   "source": [
    "### UM-Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c12b59",
   "metadata": {},
   "source": [
    "1. import parallel corpus files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b2fafcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "um_train_files = [file for file in glob.glob('./umcorpus-v1/UM-Corpus/data/*/*/*')]\n",
    "um_test_file = [file for file in glob.glob('./umcorpus-v1/UM-Corpus/data/Testing/*')]\n",
    "um_train_files.append(um_test_file[0])\n",
    "\n",
    "um_en = []\n",
    "um_zh = []\n",
    "\n",
    "for file in um_train_files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for i in range(len(lines)):\n",
    "            if i % 2 == 0:\n",
    "                um_en.append(lines[i])\n",
    "            else: \n",
    "                um_zh.append(lines[i])\n",
    "                \n",
    "                \n",
    "um_en = list(map(lambda x: x.rstrip('\\n'), um_en))\n",
    "um_zh = list(map(lambda x: x.rstrip('\\n'), um_zh))\n",
    "um_en = list(map(lambda x: x.rstrip(), um_en))\n",
    "um_zh = list(map(lambda x: x.rstrip(), um_zh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "349fb03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_vocab = list(map(lambda x: re.findall(r'[\\u4e00-\\u9fff]', x), um_zh))\n",
    "en_vocab = list(map(lambda x: re.findall(\"\\S+\", x), um_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9bccdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_vocab = [char for elem in zh_vocab for char in elem]\n",
    "en_vocab = [char for elem in en_vocab for char in elem]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9914a0b",
   "metadata": {},
   "source": [
    "### XLM-RoBERTa version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0b50b9c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b72ae7b18c8445991b3865a8b8e43a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b5627e014046329e16337597a97fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4453f210ff614aa3ab994b4c339e52c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/8.68M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "tokens = tokenizer(um_en, um_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "303faefe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9255ed85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 87,\n",
       " 174920,\n",
       " 39395,\n",
       " 23,\n",
       " 12989,\n",
       " 450,\n",
       " 87,\n",
       " 13648,\n",
       " 959,\n",
       " 18025,\n",
       " 23937,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 13129,\n",
       " 789,\n",
       " 91378,\n",
       " 7629,\n",
       " 4,\n",
       " 129164,\n",
       " 175175,\n",
       " 6711,\n",
       " 30,\n",
       " 2]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['input_ids'][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8f1311c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 88203, 2], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(random.choice(en_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "666521ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mlm(sequences, en_vocab, zh_vocab):\n",
    "    \n",
    "    ##sequences ARE tokenized\n",
    "    \n",
    "    k=24 ##count to change seed for each sequence\n",
    "    \n",
    "    masked=[]\n",
    "    labels=[]\n",
    "        \n",
    "    for sequence in sequences:\n",
    "        random.seed(k)\n",
    "        seq = sequence\n",
    "        split = seq.index(2)\n",
    "        seqlen = len(seq)\n",
    "        label = [0]*seqlen\n",
    "\n",
    "        for i in range(1, split-1):\n",
    "            cur_prob = random.random()\n",
    "            if cur_prob < 0.3333:\n",
    "                faketok = tokenizer(random.choice(en_vocab))['input_ids'][1]     \n",
    "                while seq[i] == faketok or faketok==259:\n",
    "                    faketok = tokenizer(random.choice(en_vocab))['input_ids'][1]                          \n",
    "                seq1 = seq[:i]\n",
    "                seq2 = seq[i+1:]\n",
    "                seq = seq1+[faketok]+seq2\n",
    "                label[i] = 1\n",
    "        \n",
    "        for j in range(split+2, seqlen-1):\n",
    "            cur_prob = random.random()\n",
    "            if cur_prob < 0.3333:\n",
    "                faketok = tokenizer(random.choice(zh_vocab))['input_ids'][2]\n",
    "                if seq[j] == faketok:\n",
    "                    faketok = tokenizer(random.choice(zh_vocab))['input_ids'][2]                          \n",
    "                seq1 = seq[:j]\n",
    "                seq2 = seq[j+1:]\n",
    "                seq = seq1+[faketok]+seq2\n",
    "                label[j] = 1\n",
    "        \n",
    "        masked.append(seq)\n",
    "        labels.append(label)\n",
    "                \n",
    "        k+=1\n",
    "    \n",
    "    return masked, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "11666591",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_tokens, labels = generate_mlm(tokens['input_ids'], en_vocab, zh_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3e347e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1401, 14922, 122395, 186, 70, 38352, 1672, 1632, 84765, 99, 56816, 5, 2, 2, 21404, 274, 6824, 28673, 684, 2003, 4058, 2003, 4671, 30, 2]\n",
      "[0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(masked_tokens[34])\n",
    "print(labels[34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b0c01867",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(masked_tokens)//2):\n",
    "    with open(f'./MLM xlm-roberta files/tokens1/um-mlm_tokens_{format(i, \"07d\")}.txt', 'w', encoding='utf-8') as f:\n",
    "        with open(f'./MLM xlm-roberta files/labels1/um-mlm_labels_{format(i, \"07d\")}.txt', 'w', encoding='utf-8') as g:\n",
    "            f.write(stringify(masked_tokens[i]))\n",
    "            g.write(stringify(labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "bad64afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(masked_tokens)//2, len(masked_tokens)):\n",
    "    with open(f'./MLM xlm-roberta files/tokens2/um-mlm_tokens_{format(i, \"07d\")}.txt', 'w', encoding='utf-8') as f:\n",
    "        with open(f'./MLM xlm-roberta files/labels2/um-mlm_labels_{format(i, \"07d\")}.txt', 'w', encoding='utf-8') as g:\n",
    "            f.write(stringify(masked_tokens[i]))\n",
    "            g.write(stringify(labels[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
