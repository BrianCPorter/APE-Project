{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56d59230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bcp6w\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.6.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.7.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import TFAutoModel, AutoTokenizer, PreTrainedTokenizerFast, TFAutoModelForMaskedLM, TFAutoModelForTokenClassification, TFMT5ForConditionalGeneration\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as kl\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "import tarfile\n",
    "import glob\n",
    "import random\n",
    "import csv\n",
    "import statistics\n",
    "\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3261a23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def destringify(string):\n",
    "    numlist = tf.strings.split(string)\n",
    "    numlist = tf.strings.to_number(numlist, out_type=tf.dtypes.int32)\n",
    "    return numlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b80e8bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_maskify(string):\n",
    "    numlist = tf.strings.split(string)\n",
    "    numlist = tf.strings.to_number(numlist, out_type=tf.dtypes.int32)\n",
    "    masklist = tf.math.not_equal(numlist, tf.constant([1]))\n",
    "    return tf.cast(masklist, tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef0d917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d3c1766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point5_accuracy(y_true, y_pred):\n",
    "    return tf.keras.metrics.BinaryAccuracy(threshold=0.5)(y_true, y_pred)\n",
    "\n",
    "def zero_accuracy(y_true, y_pred):\n",
    "    return tf.keras.metrics.BinaryAccuracy(threshold=0)(y_true, y_pred)\n",
    "\n",
    "def point25_accuracy(y_true, y_pred):\n",
    "    return tf.keras.metrics.BinaryAccuracy(threshold=0.25)(y_true, y_pred)\n",
    "\n",
    "def point75_accuracy(y_true, y_pred):\n",
    "    return tf.keras.metrics.BinaryAccuracy(threshold=0.75)(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dbbe2a",
   "metadata": {},
   "source": [
    "# Pretrain on UM Dataset with MLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5339ed3f",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9f97919",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_files = [file for file in glob.glob('./MLM xlm-roberta files/tokens*/*')]\n",
    "label_files = [file for file in glob.glob('./MLM xlm-roberta files/labels*/*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0762bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_file_ds = tf.data.Dataset.list_files(token_files, seed=35)\n",
    "attention_file_ds = tf.data.Dataset.list_files(token_files, seed=35)\n",
    "label_file_ds = tf.data.Dataset.list_files(label_files, seed=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3774944",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ds = tf.data.TextLineDataset(token_file_ds, num_parallel_reads=tf.data.experimental.AUTOTUNE)\n",
    "attention_ds = tf.data.TextLineDataset(attention_file_ds, num_parallel_reads=tf.data.experimental.AUTOTUNE)\n",
    "label_ds = tf.data.TextLineDataset(label_file_ds, num_parallel_reads=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc5e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ds = token_ds.map(destringify)\n",
    "attention_ds = attention_ds.map(attention_maskify)\n",
    "label_ds = label_ds.map(destringify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "920f3447",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dataset = tf.data.Dataset.zip((token_ds, attention_ds))\n",
    "dataset = tf.data.Dataset.zip((x_dataset, label_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba0b3c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset = dataset.filter(lambda x, y: tf.size(x)<=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3aa008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset = filtered_dataset.shuffle(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8912581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_dataset = filtered_dataset.padded_batch(24, padding_values = ((1, 0),0)).prefetch(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9557804a",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d923931b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFXLMRobertaForTokenClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFXLMRobertaForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model = transformers.TFXLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=1, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac7fb0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tfxlm_roberta_for_token_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " roberta (TFRobertaMainLayer  multiple                 277453056 \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  769       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 277,453,825\n",
      "Trainable params: 277,453,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a005f30",
   "metadata": {},
   "source": [
    "### Pretrain Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbe36ef",
   "metadata": {},
   "source": [
    "Step 1: Train classifier layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81dd24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.layers[0].trainable=False\n",
    "base_model.layers[1].trainable=False\n",
    "\n",
    "base_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4bd05ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86655/86655 [==============================] - 10278s 118ms/step - loss: 0.2523 - binary_accuracy: 0.8712\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x225b61c95e0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.fit(pad_dataset, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00df8773",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: second epoch appears to be unnecessary. Started at 0.25 loss. Looking like it'll end up close to that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8204f03",
   "metadata": {},
   "source": [
    "Step 2: Fine-tune primary model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e658cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.layers[0].trainable=True\n",
    "base_model.layers[1].trainable=True\n",
    "\n",
    "base_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['binary_accuracy', matthews_correlation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ab42ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86655/86655 [==============================] - 32009s 369ms/step - loss: 0.0578 - binary_accuracy: 0.9752 - matthews_correlation: 0.8961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x222fa47ef70>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.fit(pad_dataset, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "903d2cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86655/86655 [==============================] - 31567s 364ms/step - loss: 0.0460 - binary_accuracy: 0.9806 - matthews_correlation: 0.9194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22628df4640>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-6), loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['binary_accuracy', matthews_correlation])\n",
    "base_model.fit(pad_dataset, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a7f532b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86655/86655 [==============================] - 31640s 365ms/step - loss: 0.0423 - binary_accuracy: 0.9823 - matthews_correlation: 0.9263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2268fc9fdf0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6), loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['binary_accuracy', matthews_correlation])\n",
    "base_model.fit(pad_dataset, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84b8bcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn while saving (showing 5 of 1040). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: XLM-Roberta_take_3_WHOLE_MODEL_pretrained_only\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: XLM-Roberta_take_3_WHOLE_MODEL_pretrained_only\\assets\n"
     ]
    }
   ],
   "source": [
    "base_model.save_weights('XLM-Roberta_take_3_WEIGHTS_pretrained_only')\n",
    "base_model.compile(optimizer='adam', loss=None)\n",
    "base_model.save('XLM-Roberta_take_3_WHOLE_MODEL_pretrained_only')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b18841f",
   "metadata": {},
   "source": [
    "# Train model on WMT data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d61b67a",
   "metadata": {},
   "source": [
    "### Build WMT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a921fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(string):\n",
    "    x = string.split()\n",
    "    out = [y=='BAD' for y in x]\n",
    "    \n",
    "    return np.asarray(out).astype(int)\n",
    "\n",
    "def get_zh_word_labels(string):\n",
    "    x = string.split()\n",
    "    out = [y=='BAD' for y in x]\n",
    "    word = [out[i] for i in range(len(out)) if i%2==1]\n",
    "    \n",
    "    return np.asarray(word).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d32d50bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bcp6w\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\util\\_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('./WMT2021 Data/Extracted_WMT2021_data/en-zh-train/train.src', sep=\"/n\", header=None, names=[\"Source\"])\n",
    "train_df['Target'] = pd.read_csv('./WMT2021 Data/Extracted_WMT2021_data/en-zh-train/train.mt', sep=\"/n\", header=None)\n",
    "train_df['Post Edits'] = pd.read_csv('./WMT2021 Data/Extracted_WMT2021_data/en-zh-train/train.pe', sep=\"/n\", header=None)\n",
    "train_df['Source Tags'] = pd.read_csv('./WMT2021 Data/Extracted_WMT2021_data/en-zh-train/train.source_tags', sep=\"/n\", header=None)\n",
    "train_df[\"Target Tags\"] = pd.read_csv('./WMT2021 Data/Extracted_WMT2021_data/en-zh-train/train.tags', sep=\"/n\", header=None)\n",
    "\n",
    "dev_df = df = pd.read_csv('./WMT2021 Data/Extracted_WMT2021_data/en-zh-dev/dev.src', sep=\"/n\", header=None, names=[\"Source\"])\n",
    "dev_df['Target'] = pd.read_csv('./WMT2021 Data/Extracted_WMT2021_data/en-zh-dev/dev.mt', sep=\"/n\", header=None)\n",
    "dev_df['Post Edits'] = pd.read_csv('./WMT2021 Data/Extracted_WMT2021_data/en-zh-dev/dev.pe', sep=\"/n\", header=None)\n",
    "dev_df['Source Tags'] = pd.read_csv('./WMT2021 Data/Extracted_WMT2021_data/en-zh-dev/dev.source_tags', sep=\"/n\", header=None)\n",
    "dev_df[\"Target Tags\"] = pd.read_csv('./WMT2021 Data/Extracted_WMT2021_data/en-zh-dev/dev.tags', sep=\"/n\", header=None)\n",
    "\n",
    "multi_tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "train_input = [train_df['Source'][i] + multi_tokenizer.sep_token + train_df['Target'][i] for i in range(len(train_df['Source']))]\n",
    "dev_input = [dev_df['Source'][i] + multi_tokenizer.sep_token + dev_df['Target'][i] for i in range(len(dev_df['Source']))]\n",
    "\n",
    "train_en_labels = train_df[\"Source Tags\"].map(get_labels)\n",
    "train_zh_labels = train_df[\"Target Tags\"].map(get_zh_word_labels)\n",
    "\n",
    "dev_en_labels = dev_df[\"Source Tags\"].map(get_labels)\n",
    "dev_zh_labels = dev_df[\"Target Tags\"].map(get_zh_word_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bb6963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en_split = train_df[\"Source\"].map(lambda x: x.split())\n",
    "train_zh_split = train_df[\"Target\"].map(lambda x: x.split())\n",
    "\n",
    "dev_en_split = dev_df[\"Source\"].map(lambda x: x.split())\n",
    "dev_zh_split = dev_df[\"Target\"].map(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04a8bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(train_df.shape[0]):\n",
    "    assert len(train_en_split[i]) == len(train_en_labels[i])\n",
    "    assert len(train_zh_split[i]) == len(train_zh_labels[i]), print(i)\n",
    "    \n",
    "for i in range(dev_df.shape[0]):\n",
    "    assert len(dev_en_split[i]) == len(dev_en_labels[i])\n",
    "    assert len(dev_zh_split[i]) == len(dev_zh_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "764a43d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = [train_en_split[i] + [multi_tokenizer.eos_token] + train_zh_split[i] for i in range(len(train_en_split))]\n",
    "dev_input = [dev_en_split[i] + [multi_tokenizer.eos_token] + dev_zh_split[i] for i in range(len(dev_en_split))]\n",
    "\n",
    "train_toks = multi_tokenizer(train_input, max_length=256, padding='max_length', truncation=True, is_split_into_words=True, return_tensors='tf')\n",
    "dev_toks = multi_tokenizer(dev_input, max_length=256, padding='max_length', truncation=True, is_split_into_words=True,  return_tensors='tf')\n",
    "\n",
    "train_labels = [list(train_en_labels[i]) + [0] + list(train_zh_labels[i]) for i in range(len(train_en_labels))]\n",
    "dev_labels = [list(dev_en_labels[i]) + [0] + list(dev_zh_labels[i]) for i in range(len(dev_en_labels))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "440ff4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_labels(labels, tokens):\n",
    "    ## converts WMT's word-labels into token-labels\n",
    "    new_labels = np.zeros_like(tokens['input_ids'])\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        word_ids = tokens.word_ids(i)\n",
    "        cur_labels = labels[i]\n",
    "        for j in range(len(word_ids)):\n",
    "            if word_ids[j] != None:\n",
    "                new_labels[i,j]= cur_labels[word_ids[j]]\n",
    "    \n",
    "    return new_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f36b044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_expanded_labels = expand_labels(train_labels, train_toks)\n",
    "dev_expanded_labels = expand_labels(dev_labels, dev_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a6783cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt_train_ids = tf.data.Dataset.from_tensor_slices(train_toks['input_ids'])\n",
    "wmt_train_attention = tf.data.Dataset.from_tensor_slices(train_toks['attention_mask'])\n",
    "wmt_train_labels = tf.data.Dataset.from_tensor_slices(train_expanded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "062b99a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt_train_ds = tf.data.Dataset.zip((wmt_train_ids, wmt_train_attention))\n",
    "wmt_train_ds = tf.data.Dataset.zip((wmt_train_ds, wmt_train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc77a47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt_dev_ids = tf.data.Dataset.from_tensor_slices(dev_toks['input_ids'])\n",
    "wmt_dev_attention = tf.data.Dataset.from_tensor_slices(dev_toks['attention_mask'])\n",
    "wmt_dev_labels = tf.data.Dataset.from_tensor_slices(dev_expanded_labels)\n",
    "\n",
    "wmt_dev_ds = tf.data.Dataset.zip((wmt_dev_ids, wmt_dev_attention))\n",
    "wmt_dev_ds = tf.data.Dataset.zip((wmt_dev_ds, wmt_dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "869cc6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt_train_ds = wmt_train_ds.shuffle(1000).batch(12)\n",
    "wmt_dev_ds = wmt_dev_ds.shuffle(1000).batch(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5e622a",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5adc3990",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='model_take2.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_matthews_correlation',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=2, min_lr=1e-8)\n",
    "\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16436ac0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/48\n",
      "584/584 [==============================] - 288s 471ms/step - loss: 0.1669 - binary_accuracy: 0.9107 - matthews_correlation: 0.1992 - val_loss: 0.1274 - val_binary_accuracy: 0.9405 - val_matthews_correlation: 0.1958 - lr: 6.0000e-06\n",
      "Epoch 2/48\n",
      "584/584 [==============================] - 272s 466ms/step - loss: 0.1386 - binary_accuracy: 0.9235 - matthews_correlation: 0.4002 - val_loss: 0.1241 - val_binary_accuracy: 0.9445 - val_matthews_correlation: 0.4093 - lr: 6.0000e-06\n",
      "Epoch 3/48\n",
      "584/584 [==============================] - 271s 464ms/step - loss: 0.1316 - binary_accuracy: 0.9296 - matthews_correlation: 0.4758 - val_loss: 0.1203 - val_binary_accuracy: 0.9470 - val_matthews_correlation: 0.3927 - lr: 6.0000e-06\n",
      "Epoch 4/48\n",
      "584/584 [==============================] - 271s 465ms/step - loss: 0.1252 - binary_accuracy: 0.9347 - matthews_correlation: 0.5297 - val_loss: 0.1247 - val_binary_accuracy: 0.9445 - val_matthews_correlation: 0.4206 - lr: 6.0000e-06\n",
      "Epoch 5/48\n",
      "584/584 [==============================] - 271s 464ms/step - loss: 0.1180 - binary_accuracy: 0.9405 - matthews_correlation: 0.5825 - val_loss: 0.1283 - val_binary_accuracy: 0.9440 - val_matthews_correlation: 0.4251 - lr: 6.0000e-06\n",
      "Epoch 6/48\n",
      "584/584 [==============================] - 271s 463ms/step - loss: 0.1083 - binary_accuracy: 0.9466 - matthews_correlation: 0.6338 - val_loss: 0.1353 - val_binary_accuracy: 0.9441 - val_matthews_correlation: 0.4219 - lr: 1.2000e-06\n",
      "Epoch 7/48\n",
      "584/584 [==============================] - 270s 463ms/step - loss: 0.1061 - binary_accuracy: 0.9482 - matthews_correlation: 0.6474 - val_loss: 0.1363 - val_binary_accuracy: 0.9437 - val_matthews_correlation: 0.4190 - lr: 1.2000e-06\n",
      "Epoch 8/48\n",
      "584/584 [==============================] - 270s 463ms/step - loss: 0.1041 - binary_accuracy: 0.9496 - matthews_correlation: 0.6600 - val_loss: 0.1372 - val_binary_accuracy: 0.9437 - val_matthews_correlation: 0.4237 - lr: 2.4000e-07\n",
      "Epoch 9/48\n",
      "584/584 [==============================] - 269s 461ms/step - loss: 0.1035 - binary_accuracy: 0.9500 - matthews_correlation: 0.6638 - val_loss: 0.1379 - val_binary_accuracy: 0.9435 - val_matthews_correlation: 0.4203 - lr: 2.4000e-07\n",
      "Epoch 10/48\n",
      "584/584 [==============================] - 271s 464ms/step - loss: 0.1033 - binary_accuracy: 0.9502 - matthews_correlation: 0.6646 - val_loss: 0.1378 - val_binary_accuracy: 0.9435 - val_matthews_correlation: 0.4289 - lr: 4.8000e-08\n",
      "Epoch 11/48\n",
      "584/584 [==============================] - 269s 461ms/step - loss: 0.1035 - binary_accuracy: 0.9500 - matthews_correlation: 0.6643 - val_loss: 0.1376 - val_binary_accuracy: 0.9436 - val_matthews_correlation: 0.4195 - lr: 4.8000e-08\n",
      "Epoch 12/48\n",
      "584/584 [==============================] - 270s 462ms/step - loss: 0.1035 - binary_accuracy: 0.9499 - matthews_correlation: 0.6657 - val_loss: 0.1374 - val_binary_accuracy: 0.9439 - val_matthews_correlation: 0.4202 - lr: 1.0000e-08\n",
      "Epoch 13/48\n",
      "584/584 [==============================] - ETA: 0s - loss: 0.1035 - binary_accuracy: 0.9499 - matthews_correlation: 0.6625Restoring model weights from the end of the best epoch: 3.\n",
      "584/584 [==============================] - 270s 463ms/step - loss: 0.1035 - binary_accuracy: 0.9499 - matthews_correlation: 0.6625 - val_loss: 0.1373 - val_binary_accuracy: 0.9439 - val_matthews_correlation: 0.4222 - lr: 1.0000e-08\n",
      "Epoch 00013: early stopping\n"
     ]
    }
   ],
   "source": [
    "base_model.compile(optimizer=tfa.optimizers.AdamW(weight_decay=1e-6, learning_rate=6e-6), loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['binary_accuracy', matthews_correlation])\n",
    "\n",
    "history = base_model.fit(wmt_train_ds,\n",
    "               validation_data=wmt_dev_ds,\n",
    "               epochs=48,\n",
    "               verbose=1,\n",
    "                callbacks=[model_checkpoint_callback, reduce_lr_callback, early_stop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1312818e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn while saving (showing 5 of 1040). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: XLM-Roberta_take_3_WHOLE_MODEL_trained\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: XLM-Roberta_take_3_WHOLE_MODEL_trained\\assets\n"
     ]
    }
   ],
   "source": [
    "base_model.save_weights('XLM-Roberta_take_3_WEIGHTS_trained')\n",
    "base_model.compile(optimizer='adam', loss=None)\n",
    "base_model.save('XLM-Roberta_take_3_WHOLE_MODEL_trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1da439b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
